% !TeX root = ../main.tex

% Copyright (c) 2022 myl7
% SPDX-License-Identifier: CC-BY-NC-ND-4.0

\chapter{绪论}

\section{传输层协议倾向变化}

过去传输层上，在两类主流的传输层协议 TCP 和 UDP 中，相较于 UDP，TCP 保持者绝对的主要地位，专家设计上层协议时也往往在传输层选择 TCP 来承载流量，甚至是直接选择基于 TCP 的 HTTP 作为基础。
这是因为 UDP 存在一些对于当时的服务而言影响较大的几个缺点：
首先 UDP 协议不承诺送达和去重\cite{rfc768}、不保证稳定传输，在上层协议需要实现稳定传输时会加重上层协议设计的负担；
其次亦因为 UDP 不承诺送达，在公网 QoS 中 QoS 执行者常常会降低 UDP 流量权重从而保证主要的服务承载流量 TCP 流量的质量，甚至是在资源不足、硬件性能受限或是存在诸如 DDoS<TOREF UDP 反射攻击> 等安全问题的情况下直接阻断 UDP 流量，进而造成了 UDP 流量时常质量不佳的情况。
此情况尤其常见于廉价家庭宽带接入、跨运营商通信和跨国通信等环境，对普通用户服务可达性、公司服务群及内网建设等有较大的负面影响。

但在最近几年的网络协议发展中，TCP 一家独大的趋势已经逐渐开始发生变化，而 UDP 也因为其简单灵活的特点而越来越多地被选用。
一个典型的例子是 QUIC 及 HTTP/3 的出现。
HTTP 是互联网应用层中无可匹敌的主流协议。
在过去基于 TCP 的 HTTP 协议中，HTTP/1.1 和 HTTP/2 是主流选择。
其中 HTTP/1.1 由于每次请求都会打开一个新连接，资源消耗大且交互延迟高；
而 HTTP/2 则可以复用 TCP 连接，部分解决了新连接开销大的问题，但也存在队头阻塞的问题。
队头阻塞是指 TCP 拥塞控制中多个 HTTP 连接复用同一个 TCP 连接时，假如 TCP 消息队列队头的包发生了超时丢包，则此丢包会阻塞后续所有的 HTTP 连接，即使这些 HTTP 连接之间没有依赖关系。<TOREF 队头阻塞>
尽管有 BBR\cite{45646} <TOREF BBR2>等项目致力于在 TCP 发生超时丢包时优化拥塞控制策略、尽快恢复原传输速率，但仍然无法彻底解决这个问题。
而 HTTP/3 的提出便是为了解决这个问题。
HTTP/3 由 Google 主导开发，其最初是在 UDP 协议上实现了能够进行稳定传输的 QUIC 协议，后来在 2018 年 Google 提议将 HTTP over QUIC 重命名为 HTTP/3<TOREF> 以作为新一代 HTTP 标准，并于同年获得了 IETF 成员的认可<TOREF>。
尽管截至目前 HTTP/3 仍是草案状态\cite{ietf-quic-http-34}，但浏览器 Google Chrome 和 Firefox、CDN 厂商 Cloudflare 等均已部署了对 HTTP/3 的支持。
由于 HTTP/3 基于 QUIC 而 QUIC 基于 UDP，在 HTTP/3 这样的新一代上层协议中，UDP 的重要性出现了显著的上升，UDP 流量质量不佳问题的影响面也开始逐渐扩大。

不仅有 HTTP/3，WireGuard 也是一个例子。
WireGuard 是 Linux 5.6 被合并入 Linux 内核<TOREF>的、新一代 VPN 协议。
WireGuard 具有全平台、高性能、易配置、支持后量子安全<TOREF>等优点，能够极大地方便公司内网建设、自建 AS 等组网问题。
WireGuard 选择了 UDP 协议来进行节点间数据传输，以规避了 TCP over TCP 时的性能问题，并允许复用现有的 UDP over TCP 方案来降低其实现的复杂度。
此选择同样增强了 UDP 的重要性，推动了对于解决 UDP 流量质量不佳问题方法的探索。

\section{UDP over TCP 的针对优化}

为了解决 UDP 流量质量不佳的问题，目前的主流方案是利用 TCP 流量在常见 QoS 策略中的优越性，将 UDP 流量伪装为 TCP 流量，亦即 UDP over TCP。
具体实现有 GOST<TOREF gost repo> 等项目。
也有一些其他方案，例如：UDP over HTTP，从而不必自行管理 TCP 连接；UDP over WebSocket，从而提高实时性且方便兼容使用非 TCP 的流式协议作为下层协议的网络栈。

但这些 UDP over TCP 或者是其他 over 更上层协议的方案存在一些关键问题，例如：在调用操作系统接口时，协议的封装和解封装是完整通过 Linux 内核网络栈后在用户态进行的，一方面途经 Linux 内核网络栈的过程和通用 Linux API 的限制使得应用存在性能优化空间，另一方面用户态获得的原始包无法直接复用 Linux 内核网络栈的各个工具（e.g. TC、Netfilter、IP Set 等）。这些问题的出现实际上是因为目前的主流 UDP over TCP 实现仅仅是简单地将 UDP 流量在 TCP 中传输，没有针对 UDP 自由发送而 TCP 进行流式传输的差异进行特定场景的针对性优化。
而本项目 xdptun 即进行了在 UDP 与 TCP 互相转换时的针对性优化，尽管依然在协议上选择了 UDP over TCP，但是借助 Linux 可编程内核机制 eBPF，直接提前接管 L2-L4 的协议处理，从而一方面不必实现完整的下层 TCP 协议、仅需模拟 UDP 流量为 TCP 流量实现伪装，另一方面完成封装前或解封装后可以复用 Linux 内核网络栈工具，方便进一步处理并避免重复工作。

\chapter{理论基础}

\section{eBPF}

eBPF 是 Linux 内核提供的一套编程机制。
它允许开发者安全地扩展 Linux 内核功能而不需要修改内核代码、不需要重新编译内核。
eBPF 以一个二进制文件的形式被加载到内核中，首先经由 BPF verifier 校验安全性，校验通过后被解释执行。
其 runtime 带有 JIT，并允许通过 maps 进行数据持久化并与用户态程序沟通<TOREF>。

早期 BPF 作为网络包处理机制，架构较简单，功能非常受限。但从 2011 年开始 BPF 机制受到了 Linux 内核开发者的进一步开发，从而使得其功能有了较大的增强。
改进增强后的 BPF 则称为 eBPF（i.e. extended BPF）或是直接沿用 BPF 这个名称，而曾经的 BPF 则称作 cBPF（i.e. classic BPF）。\cite{10.1145/3371038}
本文中后续所有的 BPF，包括出现在特定术语中的 BPF，如无特殊解释，均指 eBPF。

为了保证外来代码的安全性、防止外来代码导致 Linux 内核崩溃，作为内核编程机制的 eBPF 强制要求一套代码安全校验机制。
首先是 eBPF 代码环境，尽管是使用 C 语言进行编程，但程序无法访问 C 语言标准库（或是更明确地说，无法访问 libc），只能调用 Linux 内核提供的一些 API 完成一些功能。
这也允许了编写的代码被编译到一个的特殊 BPF 编译目标、包含 eBPF 指令到二进制文件中从而允许 BPF 解释器执行这些 eBPF 代码。
进一步的，编译好的 eBPF 二进制文件还需要通过 BPF verifier 的校验。
BPF verifier 是 Linux 内核携带的一个 eBPF 校验器，可以使用形式化验证的方法校验 eBPF 文件是否会导致非法的内存访问，从而完美地保证此 eBPF 代码不会使得 Linux 内核崩溃。
BPF verifier 进一步地增大了用 C 语言编写 eBPF 程序时的难度，例如 C 语言中仅能从 API 提供的安全指针开始、在校验好边界的范围内生成新指针、而不允许从不安全的指针获得新指针后再进行校验。
这些最终使得 eBPF 程序编写难度很高且需要熟悉 BPF verifier 的校验机制，而这也是本项目的难点之一。
在后续的设计方案章节中将会有众多不易理解、看似冗余的代码操作，便是为了通过 BPF verifier 而做的。

\section{XDP 和 TC BPF}

XDP 和 TC BPF 是 eBPF 在网络包处理方面的两个接口。
XDP 仅工作在入方向，位于整个 Linux 内核网络栈最前端、进行于 SKB 分配之前，对每一将要进入 Linux 内核网络栈的数据帧进行处理。
挂载在 XDP 上的 eBPF 程序可以直接访问接受到的数据帧的原始数据，并可通过返回值控制此数据帧的行为、进行包过滤、修改甚至是以极低的代价避开 Linux 内核而直接将数据帧发往用户态程序。
同时由于 XDP 工作于 Linux 内核为数据包进行内存分配之前，其包处理性能非常优异，吞吐量大、延迟低。
而 TC BPF 而可以工作在出入两方向，进行于 SKB 分配之后。
由于 TC BPF 的出方向位于整个 Linux 内核网络栈最末端，在 TC BPF 出方向中可以透明地对数据包进行修改，从而避免影响其他 Linux 工具的可复用性。
本项目中我们选择了 XDP 和 TC BPF 出方向这两个接口进行开发，从而在 eBPF 中实现透明的 UDP over TCP。
