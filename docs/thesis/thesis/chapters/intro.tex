% !TeX root = ../main.tex

% Copyright (c) 2022 myl7
% SPDX-License-Identifier: CC-BY-NC-ND-4.0

\chapter{绪论}

\section{传输层协议倾向变化}

过去传输层上，在两类主流的传输层协议 TCP（Transmission Control Protocol）和 UDP（User Datagram Protocol）中，相较于 UDP，TCP 保持者绝对的主要地位，专家在设计上层协议时也往往在传输层选择 TCP 来承载流量，甚至是直接选择基于 TCP 的 HTTP（Hypertext Transfer Protocol）作为基础。
这是因为 UDP 存在一些对于当时的服务而言影响较大的几个缺点：
首先 UDP 协议不承诺送达和去重\cite{rfc768}、不保证稳定传输，在上层协议需要实现稳定传输时会加重上层协议设计的负担；
其次亦因为 UDP 不承诺送达，同时 UDP 流量常常承载一些娱乐性或是端到端的服务<TOREF>、相对而言对服务质量不太敏感，在公网 QoS 中 QoS 执行者常常会降低 UDP 流量权重从而保证主要的服务承载流量 TCP 流量的质量，甚至是在资源不足、硬件性能受限或是存在诸如 DDoS<TOREF UDP 反射攻击> 等安全问题的情况下直接阻断 UDP 流量，进而造成了 UDP 流量时常质量不佳的情况。
此情况尤其常见于廉价家庭宽带接入、跨运营商通信和跨国通信等环境中，对普通用户的服务可达性、公司服务群高可用及组织内网建设等均有较大的负面影响。

但在最近几年的网络协议发展中，TCP 一家独大的趋势已经逐渐开始发生变化，而 UDP 也因为其简单灵活的特点而越来越多地被选用。
突出的例子有 QUIC 及 HTTP/3 的出现。
HTTP 是互联网应用层中无可匹敌的主流协议。
互联网上绝大部分流量都是 HTTP 流量，HTTP 的正常工作关系到整个互联网的基本功能，其协议层的发展值得最高程度的注意。
在过去基于 TCP 的 HTTP 协议中，HTTP/1.1 和 HTTP/2 是目前的主流选择。
其中 HTTP/1.1 实现相对简单，支持也最为广泛，但其显著的一个缺点是，由于每次请求都会打开一个新连接，资源消耗大且交互延迟高；
而 HTTP/2 则可以复用 TCP 连接，部分解决了新连接开销大的问题，但也依然留下了队头阻塞的问题。
队头阻塞是指在 TCP 拥塞控制中，多个 HTTP 连接复用同一个 TCP 连接时，假如 TCP 消息队列队头的包发生了超时丢包，则此丢包会阻塞后续所有的 HTTP 连接，即使这些 HTTP 连接之间没有依赖关系。<TOREF 队头阻塞>
尽管有 BBR\cite{45646} <TOREF BBR2>等项目致力于在 TCP 发生超时丢包时优化拥塞控制策略、尽快恢复原传输速率，但仍然无法彻底地解决这个问题。
而 HTTP/3 的提出便是在从 TCP 转向 UDP 后，专家们给出的一套崭新的、行之有效的解决方案。
HTTP/3 由 Google 主导开发，其原型是在 UDP 协议上实现的、能够进行稳定传输的 QUIC 协议，后来在 2018 年 Google 提议将 HTTP over QUIC 重命名为 HTTP/3<TOREF> 以作为新一代 HTTP 标准，并于同年获得了 IETF 成员的认可。
尽管截至目前 HTTP/3 仍是草案状态\cite{ietf-quic-http-34}，但浏览器中的 Google Chrome 和 Firefox、CDN（Content Delivery Network）厂商中的 Cloudflare 等均已部署了大量的对于 HTTP/3 的支持，这表示 HTTP/3 已经实际进入生产环境了，其相关指标及发展情况急需重视。
由于 HTTP/3 基于 QUIC 而 QUIC 基于 UDP，在 HTTP/3 这样的新一代上层协议中，UDP 的重要性出现了显著的上升，UDP 流量质量不佳问题的影响面也有了可观的扩大。

不仅有 HTTP/3，WireGuard 也是一个关键的例子。
WireGuard 是 Linux 5.6 被合并入 Linux 内核主线的、新一代 VPN（Virtual Private Network）协议，从而在 OpenVPN 之外提供了一个新的 VPN 实现选择。
WireGuard 具有全平台、高性能、易配置、支持后量子安全<TOREF>等优点，能够极大地方便公司内网建设等组网问题，在当下已有了成规模的部署，并存在众多商业公司如 TailScale 等在 WireGuard 上进行了深度而复杂的开发及创新。
WireGuard 选择了 UDP 协议作为下层协议来实现节点间的数据传输，从而规避了 TCP over TCP 时冗余的 TCP 各设施如拥塞控制等带来的性能问题。
此选择使得要保证 WireGuard 的正常工作，就需要保证 UDP 在公网中的质量，也就进一步增强了 UDP 的重要性，从而正面推动了对于解决 UDP 流量质量不佳问题的方法的探索。

\section{UDP over TCP 的针对优化}

为了解决 UDP 流量质量不佳的问题，目前的主流方案是利用 TCP 流量在常见 QoS 策略中的优越性，将 UDP 流量伪装为 TCP 流量，亦即 UDP over TCP。
目前使用较多的实现有 GOST<TOREF gost repo> 等，也有众多的小型项目提供定制的特定特性。
更有一些其他另辟蹊径的设计方案例如：UDP over HTTP，从而不必实现自行管理 TCP 连接的方案；UDP over WebSocket，从而提高实时性且方便使用非 TCP 的流式协议作为下层协议的网络栈，并允许通过内容分发网络在保持实时性的前提下进行反向代理优化。

但这些 UDP over TCP 或者是其他构建与更上层协议之上的方案存在一些关键性问题，例如：
在调用操作系统接口时，协议的封装和解封装是完整通过 Linux 内核网络栈后在用户态进行的，一方面途经 Linux 内核网络栈的过程和通用 Linux API 的限制使得应用存在性能优化空间，另一方面用户态获得的原始包无法直接复用 Linux 内核网络栈的各个工具例如 TC、Netfilter、IP Set 等。
这些问题的出现实际上是因为目前的主流 UDP over TCP 实现仅仅是简单地将 UDP 流量在 TCP 中传输，没有针对 UDP 自由发送而 TCP 进行流式传输这一差异进行特定场景下的针对性优化。
而本项目 xdptun 即在 UDP 与 TCP 互相转换时进行了针对性的优化，不追求完美包装 UDP 流量为 TCP 流量，而是将 UDP 数据包低成本地、部分地伪装为 TCP 数据包的格式，从而牺牲隐蔽性换取性能和功能。
本项目在协议上依然选择了 UDP over TCP，但是具体实现上本项目使用了新兴的 Linux 可编程内核机制 eBPF，从而直接提前接管目标数据包从链路层到传输层的协议处理，进而在封装前或解封装后允许复用 Linux 内核网络栈工具，方便网络数据的进一步处理如防火墙过滤、DDoS 检测、流量转发等，避免重复制造软件轮子。

\chapter{理论基础}

\section{eBPF（extended Berkeley Packet Filter）}

eBPF 是 Linux 内核提供的一套编程机制。
它允许开发者安全地扩展 Linux 内核功能而不需要修改内核代码、不需要重新编译内核。
eBPF 以一个二进制文件的形式被加载到内核中，首先经由 BPF 检验器校验安全性，校验通过后被解释执行。
其 runtime 带有 JIT，并允许通过 maps 进行数据持久化并与用户态程序沟通<TOREF>。

早期 BPF 作为网络包处理机制，架构较简单，功能非常受限。但从 2011 年开始 BPF 机制受到了 Linux 内核开发者的进一步开发，从而使得其功能有了较大的增强。
改进增强后的 BPF 则称为 eBPF（i.e. extended BPF）或是直接沿用 BPF 这个名称，而曾经的 BPF 则称作 cBPF（i.e. classic BPF）。\cite{10.1145/3371038}
本文中后续所有的 BPF，包括出现在特定术语中的 BPF，如无特殊解释，均指 eBPF。

为了保证外来代码的安全性、防止外来代码导致 Linux 内核崩溃，作为内核编程机制的 eBPF 强制要求一套代码安全校验机制。
首先是 eBPF 代码环境，尽管是使用 C 语言进行编程，但程序无法访问 C 语言标准库（或是更明确地说，无法访问 libc），只能调用 Linux 内核提供的一些 API 完成一些功能。
这也允许了编写的代码被编译到一个的特殊 BPF 编译目标、包含 eBPF 指令到二进制文件中从而允许 BPF 解释器执行这些 eBPF 代码。
进一步的，编译好的 eBPF 二进制文件还需要通过 BPF 检验器的校验。
BPF 检验器是 Linux 内核中的一个 eBPF 检查机制，可以使用形式化验证的方法校验 eBPF 文件是否会导致非法的内存访问，从而完美地保证此 eBPF 代码不会使得 Linux 内核崩溃。
BPF 检验器进一步地增大了用 C 语言编写 eBPF 程序时的难度，例如 C 语言中仅能从 API 提供的安全指针开始、在校验好边界的范围内生成新指针、而不允许从不安全的指针获得新指针后再进行校验。
这些最终使得 eBPF 程序编写难度很高且需要熟悉 BPF 检验器的校验机制，而这也是本项目的难点之一。
在后续的设计方案章节中将会有众多不易理解、看似冗余的代码操作，便是为了通过 BPF 检验器而做的。

\section{XDP（eXpress Data Path）和 TC（Traffic Control）BPF}

XDP 和 TC BPF 是 eBPF 在网络包处理方面的两个接口。
XDP 仅工作在入方向，位于整个 Linux 内核网络栈最前端、进行于 SKB（Socket Buffer）分配之前，对每一将要进入 Linux 内核网络栈的数据帧进行处理。
挂载在 XDP 上的 eBPF 程序可以直接访问接受到的数据帧的原始数据，并可通过返回值控制此数据帧的行为、进行包过滤、修改甚至是以极低的代价避开 Linux 内核而直接将数据帧发往用户态程序。
同时由于 XDP 工作于 Linux 内核为数据包进行内存分配之前，其包处理性能非常优异，吞吐量大、延迟低。
而 TC BPF 而可以工作在出入两方向，进行于 SKB 分配之后。
由于 TC BPF 的出方向位于整个 Linux 内核网络栈最末端，在 TC BPF 出方向中可以透明地对数据包进行修改，从而避免影响其他 Linux 工具的可复用性。
本项目中我们选择了 XDP 和 TC BPF 出方向这两个接口进行开发，从而在 eBPF 中实现透明的 UDP over TCP。
