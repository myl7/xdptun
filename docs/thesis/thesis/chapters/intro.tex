% !TeX root = ../main.tex

% Copyright (c) 2022 myl7
% SPDX-License-Identifier: CC-BY-NC-ND-4.0

\chapter{绪论}

\section{选题背景及研究意义}

过去传输层上，在两类主流的传输层协议 TCP（Transmission Control Protocol）和 UDP（User Datagram Protocol）中，相较于 UDP，TCP 保持者绝对的主要地位，专家在设计上层协议时也往往在传输层选择 TCP 来承载流量，甚至是直接选择基于 TCP 的 HTTP（Hypertext Transfer Protocol）作为基础。
这是因为 UDP 存在一些对于当时的服务而言影响较大的几个缺点：
首先 UDP 协议不承诺送达和去重、不保证稳定传输\cite{rfc768}，在上层协议需要实现稳定传输时会加重上层协议设计的负担；
其次亦因为 UDP 不承诺送达，同时 UDP 流量常常承载一些娱乐性或是端到端的服务、相对而言对服务质量不太敏感\cite{5230775}，在公网 QoS（Quality of Service）中 QoS 执行者常常会降低 UDP 流量权重从而保证主要的服务承载流量 TCP 流量的质量，甚至是在资源不足、硬件性能受限或是在存在诸如 DDoS（Distributed Denial-of-service）\cite{WOS:000561707700021} 等安全问题的情况下直接阻断 UDP 流量，进而造成了 UDP 流量时常质量不佳的情况。
此情况尤其常见于廉价家庭宽带接入、跨运营商通信和跨国通信等环境中，对普通用户的服务可达性、公司服务群高可用及组织内网建设等均有较大的负面影响。

但在最近几年的网络协议发展中，TCP 一家独大的趋势已经逐渐开始发生变化，而 UDP 也因为其简单灵活的特点而越来越多地被选用。
突出的例子有 QUIC 及 HTTP/3 的出现。
HTTP 是互联网应用层中绝对的主流协议。
互联网上绝大部分流量都是 HTTP 流量，HTTP 的正常工作关系到整个互联网基本功能的正常，其协议层的发展值得最高程度的注意。
在过去基于 TCP 的 HTTP 协议中，HTTP/1.1 和 HTTP/2 是迄今的主流选择。
其中 HTTP/1.1 实现相对简单，支持也最为广泛，但其显著的一个缺点是，由于每次请求都会打开一个新连接，资源消耗大且交互延迟高；
而 HTTP/2 则可以复用 TCP 连接，部分解决了新连接开销大的问题，但也依然留有队头阻塞的问题。
队头阻塞是指在 TCP 拥塞控制中，多个 HTTP 连接复用同一个 TCP 连接时，假如 TCP 消息队列队头的包发生了超时丢包，则此丢包会阻塞后续所有的 HTTP 连接，即使这些 HTTP 连接之间没有依赖关系。
尽管有 BBR\cite{45646} 等项目致力于在 TCP 发生超时丢包时优化拥塞控制策略、尽快恢复原传输速率，但仍然无法彻底地解决这个问题。
而 HTTP/3 的提出便是在从 TCP 转向 UDP 后，专家们给出的一套崭新的、行之有效的解决方案\cite{7997281,10.1145/2695664.2695706}。
HTTP/3 由 Google 主导开发，其原型是在 UDP 协议上实现的、能够进行稳定传输的 QUIC\cite{10.1145/3098822.3098842} 协议，后来在 2018 年 Google 提议将 HTTP over QUIC 重命名为 HTTP/3 以作为新一代 HTTP 标准，并于同年获得了 IETF（Internet Engineering Task Force）成员的认可。
尽管截至目前 HTTP/3 仍是草案状态\cite{ietf-quic-http-34}，但浏览器中的 Google Chrome 和 Firefox、CDN（Content Delivery Network）厂商中的 Cloudflare 等均已部署了大量对于 HTTP/3 的支持，这表示 HTTP/3 已经实际进入生产环境，其相关指标及发展情况急需重视。
由于 HTTP/3 基于 QUIC 而 QUIC 基于 UDP，在 HTTP/3 这样的新一代上层协议中，UDP 的重要性出现了显著的上升，UDP 流量质量不佳问题的影响面也出现了扩大。

不仅有 HTTP/3，WireGuard 也是一个关键案例。
WireGuard\cite{donenfeld2017wireguard} 是 Linux 5.6 被合并入 Linux 内核代码主线的、新一代 VPN（Virtual Private Network）协议，从而在 OpenVPN 之外提供了一个新的 VPN 实现选择。
WireGuard 具有全平台、高性能、易配置、支持后量子安全\cite{9519445}等优点，能够极大地方便公司内网建设等组网问题，在当前环境下已有了成规模的部署，并存在众多商业公司如 TailScale 等在 WireGuard 上进行了深度而复杂的开发及创新。
WireGuard 选择了 UDP 协议作为下层协议来实现节点间的数据传输，从而规避了 TCP over TCP 时冗余的 TCP 设施如拥塞控制等带来的性能问题。
此选择导致，要保证 WireGuard 的正常工作，就需要保证 UDP 在公网中的质量，也就进一步增强了 UDP 的重要性，从而正面推动了对于解决 UDP 流量质量不佳问题的方法的探索。

\section{研究现状}

为了解决 UDP 流量质量不佳的问题，目前的主流方案是利用 TCP 流量在当前 QoS 策略中的优越性，将 UDP 流量伪装为 TCP 流量，亦即 UDP over TCP。
更有一些其他另辟蹊径的设计方案例如：UDP over HTTP，从而不必自行管理 TCP 连接；UDP over WebSocket，从而提高实时性且方便兼容非 TCP 的流式协议作为下层协议，并允许通过内容分发网络在保持实时性的前提下进行反向代理优化。
这些处理方案由于技术性不高在学术界中讨论不多，在工程界中则有众多现成的项目实现，例如 GOST：一个使用 Go 语言的网络隧道实现，通过简单组合各协议以实现某个协议运行在另一协议之上的效果。
亦也有一些小型的类似项目借助更高级的 Linux API 或是更丰富的周边生态支持，提供定制的某一特定特性。
但这些 UDP over TCP 或者是其他构建于更上层协议之上的方案存在一些关键问题，包括：
在调用操作系统接口时，协议的封装和解封装是完整通过 Linux 内核网络栈后在用户态进行的，一方面途经 Linux 内核网络栈的过程和通用 Linux API 的限制使得应用存在性能优化空间，另一方面用户态获得的原始包无法直接复用 Linux 内核网络栈的各个工具例如 TC、Netfilter、IP Set 等。
这些问题的出现实际上是因为目前的主流 UDP over TCP 实现仅仅是简单地将 UDP 流量在 TCP 中传输，没有针对 UDP 自由发送而 TCP 进行流式传输这一差异进行特定场景下的针对性优化。

而学术界中存在的方案则是直接处理 UDP 流量数据、通过协议解封装来达成接近 UDP over TCP 的效果，例如为 UDP 数据包添加伪 TCP 头部\cite{9670120}，从而达成较少丢包、提升 UDP 流量质量的效果。
这也是本项目实际所选用的处理方案，但相较于上述项目，本项目在插入伪 TCP 头部的具体处理流程中选用了 eBPF 来实现，并提供了一套完整可复现的处理方案，从而为这种处理方案做好进入实践运用的准备。

\section{项目工作}

相较于以往的研究项目或是工程实现，本项目 xdptun 的一个特点即在于在 UDP 与 TCP 互相转换时进行了针对性的优化，不追求完美地包装 UDP 流量为 TCP 流量，而是将 UDP 数据包低成本地、部分地伪装为 TCP 数据包的格式，从而牺牲隐蔽性换取性能和功能。
而在另一个特点上，本项目在于协议依然选择了 UDP over TCP 的同时、在具体实现上使用了新兴的 Linux 可编程内核机制 eBPF 来提供一套完整可行的具体处理方案，从而直接提前接管目标数据包从链路层到传输层的协议处理，进而在封装前或解封装后允许复用 Linux 内核网络栈工具，方便网络数据的进一步处理如防火墙过滤、DDoS 检测、流量转发等，避免重复制造软件轮子。

\chapter{理论基础}

\section{eBPF（extended Berkeley Packet Filter）}

eBPF\cite{10.1145/3371038} 是 Linux 内核提供的一套编程机制。
它允许开发者安全地扩展 Linux 内核功能而不需要修改内核代码、不需要重新编译内核。
eBPF 以一个二进制文件的形式被加载到内核中，首先经由 BPF 检验器校验安全性，校验通过后被解释执行。
其运行时带有 JIT（Just In Time）支持，并允许通过 maps 这一功能进行数据持久化并与用户态程序沟通。

早期 BPF 作为网络包处理机制，架构较简单，功能非常受限。但从 2011 年开始 BPF 机制受到了 Linux 内核开发者的进一步开发，从而使得其功能有了较大的增强。
改进增强后的 BPF 被称为 eBPF（i.e. extended BPF）或是直接沿用 BPF 这个名称，而曾经的 BPF 则被称作 cBPF（i.e. classic BPF）\cite{10.1145/3371038}。
特此声明，本文后续所有的 BPF，包括出现在特定术语中的 BPF，如无特殊解释，均指 eBPF。

为了保证外来代码的安全性、防止外来代码导致 Linux 内核崩溃，作为内核编程机制的 eBPF 强制要求一套代码安全校验机制。
首先是 eBPF 代码环境，尽管是使用 C 语言进行编程，但程序无法访问 C 语言标准库，亦即无法访问 libc，只能调用 Linux 内核提供的一些 API 和编译器提供的内联函数来完成一些功能。
这也需要源代码被编译到一个特殊的 BPF 编译目标、存储 eBPF 指令到一份二进制文件中以允许 BPF 解释器执行这些 eBPF 代码。
编译好的 eBPF 二进制文件下一步还需要通过 BPF 检验器的校验。
BPF 检验器是 Linux 内核中的一套 eBPF 检查机制，可以使用形式化验证的方法校验 eBPF 程序是否会导致非法的内存访问，从而完美地保证此 eBPF 代码不会使得 Linux 内核崩溃。
BPF 检验器进一步增大了用 C 语言编写 eBPF 程序时的难度，例如带源代码中，仅能从 API 提供的安全指针开始、在校验好边界的范围内生成新指针、而不允许从不安全的指针获得新指针后再进行校验。
这些使得 eBPF 程序的编写难度很高\cite{8850758}，而这也正是本项目的难点之一。
在后续的设计方案章节中将会有众多不易理解、看似冗余的代码操作，便是为了通过 BPF 检验器而进行的。

\section{XDP（eXpress Data Path）和 TC（Traffic Control）BPF}

XDP 和 TC BPF 是 eBPF 在网络包处理方面的两个接口。
XDP 仅工作在入方向上，位于整个 Linux 内核网络栈最前端、进行于 SKB（Socket Buffer）分配之前，对每一将要进入 Linux 内核网络栈的数据帧进行处理。
挂载在 XDP 上的 eBPF 程序可以直接访问接收到的数据帧的原始数据，并可通过返回值来控制此数据帧的行为、进行包过滤、修改等，甚至是以极低的代价避开 Linux 内核而直接将数据帧发往用户态程序。
同时由于 XDP 工作于 Linux 内核为数据包进行内存分配之前，其包处理性能非常优异，吞吐量大、延迟低\cite{8806651,8493077}。
甚至在一部分支持 XDP 卸载的网卡上，XDP 上的 eBPF 程序可以卸载到网卡中执行，从而进一步提升包处理性能\cite{kicinski2016ebpf}。
XDP 作为一套绕过 Linux 内核网络栈或是对将要进入 Linux 内核网络栈的数据帧进行修改的新兴方案，在工程中已有项目作实践验证，如 Cloudflare 公司部署的 DDoS 应对系统\cite{bertin2017xdp}以及 Cilium 容器安全项目等。

而 TC BPF 则可以工作在出入两方向上，其入方向进行于 SKB 分配之后。
由于 TC BPF 的出方向位于整个 Linux 内核网络栈最末端，仿照 XDP 在入方向上的功能，在 TC BPF 出方向中也可以透明地对数据包进行修改，从而避免在进行出方向网络包数据处理时影响其他 Linux 网络工具的可复用性。
本项目中我们即选择了 XDP 和 TC BPF 出方向这两个接口进行开发，从而在 eBPF 中实现了透明的 UDP over TCP。
